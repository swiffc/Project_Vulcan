"""
Advanced Token Optimizer - ULTIMATE Edition
Implements ALL cost-saving strategies for Claude API.

Strategies:
1. Anthropic Prompt Caching (90% savings on cached tokens)
2. Dynamic max_tokens by task type
3. Conversation history trimming
4. System prompt compression
5. Streaming with early termination
6. Batch API for non-urgent tasks (50% savings)

Packages Used: anthropic (already installed)
"""

import logging
import re
import hashlib
from typing import List, Dict, Optional, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum

logger = logging.getLogger("core.token-optimizer-v2")


class CacheStrategy(Enum):
    """When to use Anthropic's native prompt caching."""
    ALWAYS = "always"           # Cache system prompt every call
    LONG_CONTEXT = "long"       # Only cache if > 1024 tokens
    NEVER = "never"             # Don't use prompt caching


@dataclass
class OptimizedRequest:
    """Fully optimized request ready for Claude API."""
    messages: List[Dict[str, Any]]
    system: List[Dict[str, Any]]  # Structured for cache_control
    max_tokens: int
    estimated_input_tokens: int
    estimated_savings_percent: float
    cache_enabled: bool = False
    batch_eligible: bool = False


class TokenOptimizerV2:
    """
    Ultimate token optimizer with Anthropic-native features.
    
    Usage:
        optimizer = TokenOptimizerV2()
        optimized = optimizer.optimize(
            messages=conversation,
            system_prompt=agent_prompt,
            task_type="trading_query",
            enable_cache=True
        )
        
        # Use with Anthropic API
        response = client.messages.create(
            model=optimized.model,
            system=optimized.system,  # Structured with cache_control
            messages=optimized.messages,
            max_tokens=optimized.max_tokens
        )
    """
    
    # Token budgets by task type (output tokens)
    TOKEN_BUDGETS = {
        "greeting": 128,
        "status_check": 256,
        "simple_query": 256,
        "yes_no": 64,
        "cad_command": 512,
        "explanation": 1024,
        "trading_query": 1024,
        "trade_review": 2048,
        "audit": 2048,
        "full_analysis": 4096,
        "weekly_review": 4096,
        "code_generation": 2048,
        "default": 1024,
    }
    
    # History limits by complexity
    HISTORY_LIMITS = {
        "simple": 2,      # Just last exchange
        "moderate": 4,    # Last 2 exchanges
        "complex": 8,     # Last 4 exchanges
        "default": 4,
    }
    
    # Minimum tokens for Anthropic cache (model dependent)
    MIN_CACHE_TOKENS = {
        "claude-sonnet-4-20250514": 1024,
        "claude-3-haiku-20240307": 2048,
        "default": 1024
    }
    
    def __init__(self):
        self._prompt_hashes: Dict[str, str] = {}  # Track what we've cached
        
    def optimize(
        self,
        messages: List[Dict[str, str]],
        system_prompt: str,
        task_type: str = "default",
        complexity: str = "moderate",
        enable_cache: bool = True,
        model: str = "claude-sonnet-4-20250514"
    ) -> OptimizedRequest:
        """
        Fully optimize a request for minimal cost.
        
        Args:
            messages: Conversation history
            system_prompt: Agent's system prompt
            task_type: Type of task (for token budget)
            complexity: Query complexity (for history limit)
            enable_cache: Use Anthropic's native prompt caching
            model: Target model (affects cache minimums)
        """
        original_tokens = self._estimate_tokens(messages, system_prompt)
        
        # 1. Trim conversation history
        trimmed_messages = self._trim_history(messages, complexity)
        
        # 2. Compress and structure system prompt
        compressed_prompt = self._compress_prompt(system_prompt)
        
        # 3. Determine if caching is beneficial
        cache_enabled = False
        system_structured = self._structure_system_prompt(compressed_prompt, enable_cache=False)
        
        if enable_cache:
            prompt_tokens = len(compressed_prompt) // 4
            min_tokens = self.MIN_CACHE_TOKENS.get(model, 1024)
            
            if prompt_tokens >= min_tokens:
                system_structured = self._structure_system_prompt(compressed_prompt, enable_cache=True)
                cache_enabled = True
                logger.info(f"ðŸ”¥ Prompt caching ENABLED ({prompt_tokens} tokens)")
        
        # 4. Get optimal max_tokens
        max_tokens = self.TOKEN_BUDGETS.get(task_type, self.TOKEN_BUDGETS["default"])
        
        # 5. Check batch eligibility (non-urgent tasks)
        batch_eligible = task_type in ["weekly_review", "audit", "full_analysis"]
        
        # 6. Calculate savings
        optimized_tokens = self._estimate_tokens(trimmed_messages, compressed_prompt)
        base_savings = ((original_tokens - optimized_tokens) / original_tokens * 100) if original_tokens > 0 else 0
        
        # Add cache savings estimate (90% on system prompt if cached)
        cache_savings = 0
        if cache_enabled:
            system_tokens = len(compressed_prompt) // 4
            # After first call, cached reads are 90% cheaper
            cache_savings = (system_tokens / original_tokens) * 90 if original_tokens > 0 else 0
        
        total_savings = min(base_savings + cache_savings, 95)  # Cap at 95%
        
        if total_savings > 5:
            logger.info(f"ðŸ’° Optimized: {original_tokens} â†’ {optimized_tokens} tokens ({total_savings:.0f}% potential savings)")
        
        return OptimizedRequest(
            messages=trimmed_messages,
            system=system_structured,
            max_tokens=max_tokens,
            estimated_input_tokens=optimized_tokens,
            estimated_savings_percent=total_savings,
            cache_enabled=cache_enabled,
            batch_eligible=batch_eligible
        )
    
    def _structure_system_prompt(
        self, 
        prompt: str, 
        enable_cache: bool = True
    ) -> List[Dict[str, Any]]:
        """
        Structure system prompt for Anthropic API with optional cache_control.
        
        This is the KEY to Anthropic's native prompt caching!
        """
        if not prompt:
            return []
            
        content = {"type": "text", "text": prompt}
        
        if enable_cache:
            content["cache_control"] = {"type": "ephemeral"}
            
        return [content]
    
    def _trim_history(
        self, 
        messages: List[Dict[str, str]], 
        complexity: str
    ) -> List[Dict[str, str]]:
        """Aggressively trim conversation history."""
        limit = self.HISTORY_LIMITS.get(complexity, self.HISTORY_LIMITS["default"])
        
        if len(messages) <= limit:
            return messages
        
        # For long conversations, keep summary + recent
        if len(messages) > 10:
            # Could inject a summary here in production
            return messages[-limit:]
        
        return messages[-limit:]
    
    def _compress_prompt(self, prompt: str) -> str:
        """Compress system prompt to reduce tokens."""
        if not prompt:
            return prompt
        
        compressed = prompt
        
        # Remove excessive whitespace
        compressed = re.sub(r'\n{3,}', '\n\n', compressed)
        compressed = re.sub(r' {2,}', ' ', compressed)
        compressed = re.sub(r'\t+', ' ', compressed)
        
        # Remove markdown decorations that don't add value
        compressed = re.sub(r'-{4,}', '---', compressed)
        compressed = re.sub(r'={4,}', '===', compressed)
        compressed = re.sub(r'\*{3,}', '**', compressed)
        
        # Remove empty list items
        compressed = re.sub(r'\n\s*[-*]\s*\n', '\n', compressed)
        
        # Compress common verbose phrases
        replacements = [
            (r'You are an AI assistant', 'You are'),
            (r'Please note that', 'Note:'),
            (r'It is important to', 'Important:'),
            (r'Make sure to', 'Ensure'),
            (r'In order to', 'To'),
            (r'As well as', 'and'),
            (r'Due to the fact that', 'Because'),
            (r'In the event that', 'If'),
            (r'At this point in time', 'Now'),
            (r'For the purpose of', 'For'),
        ]
        
        for pattern, replacement in replacements:
            compressed = re.sub(pattern, replacement, compressed, flags=re.IGNORECASE)
        
        return compressed.strip()
    
    def _estimate_tokens(
        self, 
        messages: List[Dict[str, str]], 
        prompt: str
    ) -> int:
        """Estimate token count (1 token â‰ˆ 4 chars for English)."""
        total = len(prompt)
        for msg in messages:
            content = msg.get("content", "")
            if isinstance(content, str):
                total += len(content)
            elif isinstance(content, list):
                for item in content:
                    if isinstance(item, dict):
                        total += len(str(item.get("text", "")))
        return total // 4
    
    def get_task_type(self, message: str, agent_type: str = "general") -> str:
        """Infer optimal task type from message content."""
        lower = message.lower().strip()
        
        # Very short messages
        if len(lower) < 20:
            if re.match(r'^(hi|hello|hey|yo|sup)', lower):
                return "greeting"
            if re.match(r'^(yes|no|ok|okay|sure|thanks|thx)', lower):
                return "yes_no"
        
        # Question patterns
        if any(kw in lower for kw in ["status", "health", "check", "ping"]):
            return "status_check"
        
        if any(kw in lower for kw in ["analyze", "analysis", "comprehensive", "detailed", "in-depth"]):
            return "full_analysis"
        
        if any(kw in lower for kw in ["review", "evaluate", "assess"]):
            if agent_type == "trading":
                return "trade_review"
            return "audit"
        
        if any(kw in lower for kw in ["weekly", "summary", "report"]):
            return "weekly_review"
        
        if any(kw in lower for kw in ["code", "script", "function", "implement"]):
            return "code_generation"
        
        # Agent-specific defaults
        if agent_type == "trading":
            return "trading_query"
        if agent_type == "cad":
            return "cad_command"
        if agent_type == "inspector":
            return "audit"
        
        return "default"
    
    def get_complexity(self, message: str) -> str:
        """Determine query complexity for history trimming."""
        lower = message.lower()
        word_count = len(message.split())
        
        # Simple indicators
        if word_count < 10:
            return "simple"
        
        # Complex indicators
        complex_keywords = [
            "analyze", "compare", "explain", "why", "how does",
            "relationship", "implications", "comprehensive"
        ]
        
        if any(kw in lower for kw in complex_keywords) or word_count > 50:
            return "complex"
        
        return "moderate"
    
    def prepare_for_batch(
        self,
        requests: List[Tuple[List[Dict], str, str]]
    ) -> List[Dict[str, Any]]:
        """
        Prepare multiple requests for Batch API (50% savings).
        
        Args:
            requests: List of (messages, system_prompt, custom_id) tuples
            
        Returns:
            List of batch request objects
        """
        batch_requests = []
        
        for messages, system_prompt, custom_id in requests:
            optimized = self.optimize(
                messages=messages,
                system_prompt=system_prompt,
                enable_cache=False  # Batch API doesn't use prompt caching
            )
            
            batch_requests.append({
                "custom_id": custom_id,
                "params": {
                    "model": "claude-sonnet-4-20250514",
                    "max_tokens": optimized.max_tokens,
                    "system": optimized.system,
                    "messages": optimized.messages
                }
            })
        
        return batch_requests


# Singleton
_optimizer_v2: Optional[TokenOptimizerV2] = None

def get_token_optimizer_v2() -> TokenOptimizerV2:
    global _optimizer_v2
    if _optimizer_v2 is None:
        _optimizer_v2 = TokenOptimizerV2()
    return _optimizer_v2
